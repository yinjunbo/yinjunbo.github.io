<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: November 19, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.042e26407c9e383d96a1f26d6787c686.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Junbo Yin 阴俊博"><meta name=description content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://yinjunbo.github.io/><link rel=canonical href=https://yinjunbo.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://yinjunbo.github.io/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Junbo Yin"><meta property="og:url" content="https://yinjunbo.github.io/"><meta property="og:title" content="Junbo Yin"><meta property="og:description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><meta property="og:image" content="https://yinjunbo.github.io/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-09-24T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://yinjunbo.github.io?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://yinjunbo.github.io"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script><link rel=alternate href=/index.xml type=application/rss+xml title="Junbo Yin"><title>Junbo Yin</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Junbo Yin</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Junbo Yin</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#news data-target=#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#activities data-target=#activities><span>Activities</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href="https://scholar.google.com/citations?user=OiEQrqUAAAAJ&amp;hl=zh-CN" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about-biography"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_hucb526607eb17f3d0a8e396b8a6214a1e_1646048_270x270_fill_q75_lanczos_center.jpg alt="Junbo Yin 阴俊博"><div class=portrait-title><h2>Junbo Yin 阴俊博</h2><h3>Ph.D. Candidate</h3><h3><a href=https://english.bit.edu.cn/ target=_blank rel=noopener><span>Beijing Institute of Technology</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:yinjunbocn@gmail.com aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://github.com/yinjunbo target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=OiEQrqUAAAAJ&amp;hl=zh-CN" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p style=text-align:justify>Junbo Yin is a Ph.D. candidate in Beijing Lab of Intelligent Information Technology at Beijing Institute of Technology (BIT), under the supervision of <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&amp;hl=zh-TW" target=_blank rel=noopener>Prof. Jianbing Shen</a>. He is also a visiting Ph.D. at EPFL, supervised by <a href="https://scholar.google.com/citations?user=-Ve9sJ0AAAAJ&amp;hl=zh-TW" target=_blank rel=noopener>Prof. Pascal Frossard</a>. His research interests include 3D object detection, self-supervised learning, and 3D domain adaptation. He has published 15 conference and journal papers such as CVPR, ICCV, ECCV and TPAMI, with oevr 950 citations on <a href="https://scholar.google.com/citations?user=OiEQrqUAAAAJ&amp;hl=zh-CN" target=_blank rel=noopener>Google Scholar</a>. He has obtained many prestigious scholarships and honorary titles, including Excellent Graduate Student, Special Grad Scholarship and Huawei Scholarship. Additionally, he has been awarded by the Zhejiang Lab&rsquo;s International Talent Fund for Young Professionals.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests fa-ul mb-0"><li><i class="fa-li fa-solid fa-book"></i>
3D Object Detection & Tracking</li><li><i class="fa-li fa-solid fa-book"></i>
Multimodal Learning</li><li><i class="fa-li fa-solid fa-book"></i>
Transfer Learning</li><li><i class="fa-li fa-solid fa-book"></i>
Autonomous Driving</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>Ph.D. in Computer Vision, 2018</p><p class=institution>Beijing Institute of Technology</p></div></li><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>MEng in Pattern Recognition and Intelligent System, 2016</p><p class=institution>Beijing Institute of Technology</p></div></li><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>BSc in Automation, 2012</p><p class=institution>North China Electric Power University</p></div></li></ul></div></div></div></div></div></section><section id=news class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>News</h1></div><div class=col-12><center style=text-align:left><ul><li>2023.10: 🎉🎉 <strong>one paper</strong> is accepted to ICCV‘23.</li><li>2023.02: 🎉🎉 <strong>two papers</strong> are accepted to AAAI‘23.</li><li>2022.10: 🎉🎉 we are <strong>2nd</strong> in ECCV'22 <a href=https://innoviz.tech/eccv-challenge target=_blank rel=noopener>LiDAR Self-Supervised Learning Challenge</a>.</li><li>2022.10: 🎉🎉 <strong>three papers</strong> are accepted to ECCV'22.</li><li>2021.12: 🎉🎉 <strong>one papers</strong> is accepted to TPAMI.</li><li>2021.10: 🎉🎉 we are <strong>3rd</strong> in ICCV'21 <a href=https://sslad2022.github.io/pages/challenge.html target=_blank rel=noopener>SSLAD Workshop</a>.</li><li>2021.05: 🎉🎉 we are <strong>1st</strong> in ICRA'21 <a href=https://driving-olympics.ai/ target=_blank rel=noopener>AI Driving Olympics</a>.</li><li>2020.06: 🎉🎉 <strong>three papers</strong> are accepted to CVPR'20.</li></ul></center></div></div></div></section><section id=publications class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="card-simple view-card"><div class=article-metadata><div><span>Xiang Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Junbo Yin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Botian Shi</span>, <span>Yikang Li</span>, <span>Ruigang Yang</span>, <span>Jianbing Shen</span></div><span class=article-date>February, 2023
</span><span class=middot-divider></span>
<span class=pub-publication>In <em>AAAI</em></span></div><a href=/publication/aaai23lwsis/><div class=img-hover-zoom><img src=/publication/aaai23lwsis/featured_hu338c1f3e801374ac0a802ac8924065fb_605234_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="LWSIS: LiDAR-Guided Weakly Supervised Instance Segmentation for Autonomous Driving" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/aaai23lwsis/>LWSIS: LiDAR-Guided Weakly Supervised Instance Segmentation for Autonomous Driving</a></div><a href=/publication/aaai23lwsis/ class=summary-link><div class=article-style><p>We present a novel learning paradigm, LWSIS, that inherits the fruits of off-the-shelf 3D point cloud to guide the training of 2D instance segmentation models to save mask-level annotations. We advocate a new dataset nuInsSeg based on nuScenes to extend existing 3D LiDAR annotations with 2D image segmentation annotations.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ojs.aaai.org/index.php/AAAI/article/view/25228/25000 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/aaai23lwsis/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.%20com/Serenos/LWSIS target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Serenos/nuInsSeg target=_blank rel=noopener>Dataset</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Yan Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Junbo Yin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Wei Li</span>, <span>Pascal Frossard</span>, <span>Ruigang Yang</span>, <span>Jianbing Shen</span></div><span class=article-date>February, 2023
</span><span class=middot-divider></span>
<span class=pub-publication>In <em>AAAI</em></span></div><a href=/publication/aaai23ssda/><div class=img-hover-zoom><img src=/publication/aaai23ssda/featured_hu03e363b4f2548340ac004b76b3c000c4_1283827_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="SSDA3D: Semi-supervised Domain Adaptation for 3D Object Detection from Point Cloud" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/aaai23ssda/>SSDA3D: Semi-supervised Domain Adaptation for 3D Object Detection from Point Cloud</a></div><a href=/publication/aaai23ssda/ class=summary-link><div class=article-style><p>We prsent SSDA3D in this work, which is the first effort for semi-supervised domain adaptation in the context of 3D object detection. SSDA3D is achieved by a novel framework that jointly addresses inter-domain adaptation and intra-domain generalization.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ojs.aaai.org/index.php/AAAI/article/view/25370/25142 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/aaai23ssda/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.%20com/yinjunbo/SSDA3D target=_blank rel=noopener>Code</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Junbo Yin</span>, <span>Dingfu Zhou</span>, <span>Liangjun Zhang</span>, <span>Jin Fang</span>, <span>Cheng-Zhong Xu</span>, <span>Jianbing Shen</span>, <span>Wenguan Wang</span></div><span class=article-date>October, 2022
</span><span class=middot-divider></span>
<span class=pub-publication>In <em>ECCV</em></span></div><a href=/publication/eccv22self/><div class=img-hover-zoom><img src=/publication/eccv22self/featured_huf2ef780a5664fc61c657f0b2a6dcaee2_2282284_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/eccv22self/>ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection</a></div><a href=/publication/eccv22self/ class=summary-link><div class=article-style><p>We propose a proposal-level point cloud SSL framework, named ProposalContrast, that conducts proposal-wise contrastive pre-training for detection-aligned representation learning. We comprehensively demonstrate the generalizability of our model across diverse 3D detector architectures and datasets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990017.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/eccv22self/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yinjunbo/ProposalContrast target=_blank rel=noopener>Code</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Junbo Yin</span>, <span>Jin Fang</span>, <span>Dingfu Zhou</span>, <span>Liangjun Zhang</span>, <span>Cheng-Zhong Xu</span>, <span>Jianbing Shen</span>, <span>Wenguan Wang</span></div><span class=article-date>October, 2022
</span><span class=middot-divider></span>
<span class=pub-publication>In <em>ECCV</em></span></div><a href=/publication/eccv22semi/><div class=img-hover-zoom><img src=/publication/eccv22semi/featured_hu5c1355f6f10dcb05f8b806821474919a_1262879_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="Semi-supervised 3D Object Detection wit Proficient Teachers" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/eccv22semi/>Semi-supervised 3D Object Detection wit Proficient Teachers</a></div><a href=/publication/eccv22semi/ class=summary-link><div class=article-style><p>We propose ProficientTeachers for LiDAR-based 3D object detection, which is achieved by promoting the plain teacher model to proficient teachers inspired by ensemble learning. Our framework not only performs better results, but also removes the necessity of confidencebased thresholds for filtering pseudo labels.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980710.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/eccv22semi/cite.bib>Cite</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Junbo Yin</span>, <span>Jianbing Shen</span>, <span>Xin Gao</span>, <span>David J. Crandall</span>, <span>Ruigang Yang</span></div><span class=article-date>December, 2021
</span><span class=middot-divider></span>
<span class=pub-publication>TPAMI</span></div><a href=/publication/pami21vid/><div class=img-hover-zoom><img src=/publication/pami21vid/featured_hue508aa8df4c92dbfeca8bd0f0e2666d2_640046_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="Graph Neural Network and Spatiotempora Transformer Attention for 3D Video Objec Detection From Point Clouds" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/pami21vid/>Graph Neural Network and Spatiotempora Transformer Attention for 3D Video Objec Detection From Point Clouds</a></div><a href=/publication/pami21vid/ class=summary-link><div class=article-style><p>A general point cloud-based 3D video object detection framework is proposed by leveraging both short-term and long-term point cloud information. We build the proposed 3D video object detectio framework upon both anchor-based and anchor-fre 3D object detectors. Also, the proposed framework can be easily deployed in both online and offlin modes.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609569" target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pami21vid/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3125981 target=_blank rel=noopener>DOI</a></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=activities class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Activities</h1></div><div class=col-12><p><strong>Reviewer:</strong></p><ul><li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li><li>IEEE Transactions on Image Processing (TIP)</li><li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li><li>IEEE Robotics and Automation Letters (RAL)</li><li>Pattern Recognition (PR)</li><li>Neurocomputing (NEUCOM)</li></ul></center></div></div></div></section><section id=section-experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Work Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://en.inceptio.ai/ target=_blank rel=noopener><img src=/media/icons/brands/inceptio.svg width=56px height=56px alt=Inceptio loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://en.inceptio.ai/ target=_blank rel=noopener>Inceptio</a></div><div class="text-muted exp-meta">January 2022 –
January 2023
<span class=middot-divider></span>
<span>Shanghai, China</span></div></div></div><div class=card-text>Supervised by <a href="https://scholar.google.com/citations?user=yveq40QAAAAJ&amp;hl=zh-TW" target=_blank rel=noopener>Prof. Ruigang Yang</a>.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=http://research.baidu.com/ target=_blank rel=noopener><img src=/media/icons/brands/baidu.svg width=56px height=56px alt="Baidu Research" loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=http://research.baidu.com/ target=_blank rel=noopener>Baidu Research</a></div><div class="text-muted exp-meta">June 2020 –
December 2021
<span class=middot-divider></span>
<span>Beijing, China</span></div></div></div><div class=card-text>Supervised by <a href="https://scholar.google.com/citations?user=Byzk604AAAAJ&amp;hl=en" target=_blank rel=noopener>Prof. Liangjun Zhang</a> and <a href="https://scholar.google.com/citations?user=CqAQQkgAAAAJ&amp;hl=zh-CN" target=_blank rel=noopener>Prof. Wenguan Wang</a>.</div></div></div></div></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.85070d5fe00d43eaedff44310b81dc2c.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>