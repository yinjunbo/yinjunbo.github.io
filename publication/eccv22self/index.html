<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: September 29, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.042e26407c9e383d96a1f26d6787c686.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Junbo Yin 阴俊博"><meta name=description content="We propose a proposal-level point cloud SSL framework, named ProposalContrast, that conducts proposal-wise contrastive pre-training for detection-aligned representation learning. We comprehensively demonstrate the generalizability of our model across diverse 3D detector architectures and datasets."><link rel=alternate hreflang=en-us href=https://yinjunbo.github.io/publication/eccv22self/><link rel=canonical href=https://yinjunbo.github.io/publication/eccv22self/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://yinjunbo.github.io/publication/eccv22self/featured.jpg"><meta property="og:site_name" content="Junbo Yin"><meta property="og:url" content="https://yinjunbo.github.io/publication/eccv22self/"><meta property="og:title" content="ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection | Junbo Yin"><meta property="og:description" content="We propose a proposal-level point cloud SSL framework, named ProposalContrast, that conducts proposal-wise contrastive pre-training for detection-aligned representation learning. We comprehensively demonstrate the generalizability of our model across diverse 3D detector architectures and datasets."><meta property="og:image" content="https://yinjunbo.github.io/publication/eccv22self/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://yinjunbo.github.io/publication/eccv22self/"},"headline":"ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection","image":["https://yinjunbo.github.io/publication/eccv22self/featured.jpg"],"datePublished":"2022-10-01T00:00:00Z","dateModified":"2022-10-01T00:00:00Z","author":{"@type":"Person","name":"Junbo Yin"},"publisher":{"@type":"Organization","name":"Junbo Yin","logo":{"@type":"ImageObject","url":"https://yinjunbo.github.io/media/icon_hub176d598b0f976174f6ebf45074facb3_20991_192x192_fill_lanczos_center_3.png"}},"description":"We propose a proposal-level point cloud SSL framework, named ProposalContrast, that conducts proposal-wise contrastive pre-training for detection-aligned representation learning. We comprehensively demonstrate the generalizability of our model across diverse 3D detector architectures and datasets."}</script><title>ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection | Junbo Yin</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=b32574ed70a68621b81298247d199534><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Junbo Yin</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Junbo Yin</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#activities><span>Activities</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href="https://scholar.google.com/citations?user=OiEQrqUAAAAJ&amp;hl=zh-CN" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection</h1><div class=article-metadata><div><span>Junbo Yin</span>, <span>Dingfu Zhou</span>, <span>Liangjun Zhang</span>, <span>Jin Fang</span>, <span>Cheng-Zhong Xu</span>, <span>Jianbing Shen</span>, <span>Wenguan Wang</span></div><span class=article-date>October, 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990017.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/eccv22self/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/yinjunbo/ProposalContrast target=_blank rel=noopener>Code</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:436px><div style=position:relative><img src=/publication/eccv22self/featured_huf2ef780a5664fc61c657f0b2a6dcaee2_2282284_720x2500_fit_q75_h2_lanczos.webp width=720 height=436 alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Existing approaches for unsupervised point cloud pre-training are constrained to either scene-level or point/voxel-level instance discrimination. Scene-level methods tend to lose local details that are crucial for recognizing the road objects, while point/voxel-level methods inherently suffer from limited receptive field that is incapable of perceiving large objects or context environments. Considering region-level representations are more suitable for 3D object detection, we devise a new unsupervised point cloud pre-training framework, called ProposalContrast, that learns robust 3D representations by contrasting region proposals. Specifically, with an exhaustive set of region proposals sampled from each point cloud, geometric point relations within each proposal are modeled for creating expressive proposal representations. To better accommodate 3D detection properties, ProposalContrast optimizes with both inter-cluster and inter-proposal separation, i.e., sharpening the discriminativeness of proposal representations across semantic classes and object instances. The generalizability and transferability of ProposalContrast are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>ECCV 2022</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><!-- 

<div class="alert alert-note">
  <div>
    Click the <em>Cite</em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  </div>
</div>
 --><!-- 

<div class="alert alert-note">
  <div>
    Create your slides in Markdown - click the <em>Slides</em> button to check out the example.
  </div>
</div>
 --><!-- Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fyinjunbo.github.io%2Fpublication%2Feccv22self%2F&amp;text=ProposalContrast%3A+Unsupervised+Pre-training+for+LiDAR-based+3D+Object+Detection" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fyinjunbo.github.io%2Fpublication%2Feccv22self%2F&amp;t=ProposalContrast%3A+Unsupervised+Pre-training+for+LiDAR-based+3D+Object+Detection" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=ProposalContrast%3A%20Unsupervised%20Pre-training%20for%20LiDAR-based%203D%20Object%20Detection&amp;body=https%3A%2F%2Fyinjunbo.github.io%2Fpublication%2Feccv22self%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fyinjunbo.github.io%2Fpublication%2Feccv22self%2F&amp;title=ProposalContrast%3A+Unsupervised+Pre-training+for+LiDAR-based+3D+Object+Detection" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=ProposalContrast%3A+Unsupervised+Pre-training+for+LiDAR-based+3D+Object+Detection%20https%3A%2F%2Fyinjunbo.github.io%2Fpublication%2Feccv22self%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fyinjunbo.github.io%2Fpublication%2Feccv22self%2F&amp;title=ProposalContrast%3A+Unsupervised+Pre-training+for+LiDAR-based+3D+Object+Detection" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.3322c0d94f0e691b0b24c63f4c41064b.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>